% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ProtMatch2.R
\name{ProtMatch2}
\alias{ProtMatch2}
\title{ProtMatch2}
\usage{
ProtMatch2(
  Seq,
  DB,
  Cut = c("K_", "R_"),
  N.clust = get("N.clust"),
  N.reserved = 1,
  cl,
  I_eq_L = TRUE
)
}
\arguments{
\item{Seq}{Character vector of peptide sequences.}

\item{DB}{The formated protein database, with proteins IDs and sequences.}

\item{Cut}{For digestion. The character vector of amino acid(s) after which to cut. For each, provide one letter followed or preceded by underscore indicating whether the cut is C- or N-terminal, respectively (usually C-terminal). Default (for Trypsin) = c("K_", "R_")}

\item{N.clust}{A limit on the number of vCPUs to use. If left as NULL (default), uses the number of available clusters - 1, to a minimum of 1. Only used if missed > 0}

\item{N.reserved}{Default = 1. Number of reserved vCPUs the function is not to use. Note that for obvious reasons it will always use at least one. Only used if missed > 0}

\item{cl}{Already have a cluster handy? Why spend time making a new one, which on top of that may invalidate the old one. Just pass it along!}

\item{I_eq_L}{Should we consider I and L identical? Currently, by default, TRUE for both DIA and DDA: see https://github.com/vdemichev/DiaNN/discussions/1631}
}
\description{
A function to match observed peptide sequences to a protein digest:
  - Some search software do not report all proteins from the database which match a specific sequence.
  - Others, such as MaxQuant, do report weird matches for a minority of PSMs, or miss a few perfectly valid matches.
 Whilst this may be correct from their point of view, this function allows re-checking matches for consistency between software.
 This will return a data frame of matches between observed peptides and protein IDs from a formatted protein sequences database.

10/06/2025
----------
Aaand... once again, this function has been radically re-written. Again, I did not rename it, because as far as I can tell the outcome is the same!

The way the previous version used data.tables was not that great (creating un-necessarily large tables), and the function remained too slow.
I have dropped those parts and instead have focused on filtering stuff as early as possible.
I have also improved the parallelisation: now instead of using parallel::clusterExport() for large objects, the functions writes (then deletes) temporary .RDS files.
If at some point writing those should fail, we could try the following solutions:
 - using tmpfile() for those instead of a fixed-name file,
 - or allow reverting to clusterExport() if .RDS serialization doesn't work.

These changes have yielded an about 20x gain in speed, turning the function from a cumbersome monster to a rather fast one.
}
